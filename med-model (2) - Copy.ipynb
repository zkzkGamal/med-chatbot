{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30daf727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding,AdditiveAttention, GRU,Multiply, Dense, LSTM, Concatenate, TimeDistributed, Bidirectional, BatchNormalization, LayerNormalization, MultiHeadAttention , Dropout\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau , TensorBoard, CSVLogger, ModelCheckpoint\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79772165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n",
      "Data filtered\n",
      "Preprocessing done\n"
     ]
    }
   ],
   "source": [
    "# # Preprocessing function\n",
    "# def preprocess_text(txt):\n",
    "#     txt = txt.lower()\n",
    "#     txt = re.sub(r\"i'm\", \"i am\", txt)\n",
    "#     txt = re.sub(r\"he's\", \"he is\", txt)\n",
    "#     txt = re.sub(r\"she's\", \"she is\", txt)\n",
    "#     txt = re.sub(r\"that's\", \"that is\", txt)\n",
    "#     txt = re.sub(r\"what's\", \"what is\", txt)\n",
    "#     txt = re.sub(r\"where's\", \"where is\", txt)\n",
    "#     txt = re.sub(r\"\\'ll\", \" will\", txt)\n",
    "#     txt = re.sub(r\"\\'ve\", \" have\", txt)\n",
    "#     txt = re.sub(r\"\\'re\", \" are\", txt)\n",
    "#     txt = re.sub(r\"\\'d\", \" would\", txt)\n",
    "#     txt = re.sub(r\"won't\", \"will not\", txt)\n",
    "#     txt = re.sub(r\"can't\", \"can not\", txt)\n",
    "#     txt = re.sub(r\"wanna\", \"want to\", txt)\n",
    "#     txt = re.sub(r'[()\\[\\]{}]', ' ', txt)\n",
    "#     txt = re.sub(r'[-]', ' ', txt)\n",
    "#     txt = re.sub(r'[\\'\"]', '', txt)\n",
    "#     txt = re.sub(r\"[^\\w\\s]\", \"\", txt)\n",
    "#     tokens = word_tokenize(txt)\n",
    "#     # stop_words = set(stopwords.words('english'))\n",
    "#     # tokens = [word for word in tokens if word not in stop_words]\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "#     processed_text = ' '.join(tokens)\n",
    "#     return processed_text\n",
    "\n",
    "# # Load and preprocess data\n",
    "# df = pd.read_csv('./med-en-data.csv')\n",
    "# df = df.astype(str)\n",
    "# columns_to_drop = ['Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 0.2']\n",
    "# df = df.drop(columns_to_drop, axis=1)\n",
    "# df = df[:15000]\n",
    "# print('date loaded')\n",
    "\n",
    "# # Filter rows where 'Quesition' or 'Answer' have more than 223 words\n",
    "# df = df[df['Answer'].apply(lambda x: len(x.split()) <= 224)]\n",
    "# df = df[df['Quesition'].apply(lambda x: len(x.split()) <= 224)]\n",
    "\n",
    "\n",
    "# print('data filtered')\n",
    "\n",
    "# df['Quesition'] = df['Quesition'].apply(lambda x : preprocess_text(x))\n",
    "# df['Answer'] = df['Answer'].apply(lambda x : preprocess_text(x))\n",
    "\n",
    "# print('done')\n",
    "\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(txt , is_answer = False):\n",
    "    txt = txt.lower()\n",
    "    contractions = {\n",
    "        \"i'm\": \"i am\", \"he's\": \"he is\", \"she's\": \"she is\", \"that's\": \"that is\",\n",
    "        \"what's\": \"what is\", \"where's\": \"where is\", \"'ll\": \" will\", \"'ve\": \" have\",\n",
    "        \"'re\": \" are\", \"'d\": \" would\", \"won't\": \"will not\", \"can't\": \"can not\",\n",
    "        \"wanna\": \"want to\" , \"q:a\": \"q: a\" , \"healthcaremagic.comi\" :\"healthcaremagic.com i\"\n",
    "    }\n",
    "    for contraction, replacement in contractions.items():\n",
    "        txt = re.sub(contraction, replacement, txt)\n",
    "    \n",
    "    txt = re.sub(r'[^\\w\\s\\?]', '', txt)\n",
    "    # txt = re.sub(r\"healthcaremagic.comi\", \"healthcaremagic.com i\", txt)\n",
    "\n",
    "    # Remove special characters\n",
    "    # txt = re.sub(r'[()\\[\\]{}]', ' ', txt)\n",
    "    # txt = re.sub(r'[-]', ' ', txt)\n",
    "    # txt = re.sub(r'[\\'\"]', '', txt)\n",
    "    # txt = re.sub(r\"[^\\w\\s]\", \"\", txt)\n",
    "    txt = re.sub(r'[^\\w\\s\\?]', '', txt)\n",
    "\n",
    "    if not is_answer:\n",
    "        txt = re.sub(r\"q \",'', txt)\n",
    "        txt = re.sub(r'\\?{2,}','?' , txt)\n",
    "\n",
    "\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(txt)\n",
    "    \n",
    "    # Remove stopwords (customized for medical texts)\n",
    "    # stop_words = set(stopwords.words('english'))\n",
    "    # custom_stopwords = {'q'}\n",
    "    # stop_words = stop_words.union(custom_stopwords)\n",
    "    # tokens = [word for word in tokens if word not in custom_stopwords]\n",
    "    \n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Join tokens back to a single string\n",
    "    processed_text = ' '.join(tokens)\n",
    "\n",
    "    # Modify multiple-choice answers to include the format \"the answer is x: y\"\n",
    "    if is_answer:\n",
    "        match = re.match(r'([a-e]) (.*)', processed_text)\n",
    "        if match:\n",
    "            processed_text = f\"the answer is {match.group(1)} {match.group(2)}\"\n",
    "    if processed_text.endswith(','):\n",
    "        processed_text = processed_text[:-2]\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "    \n",
    "# Load and preprocess data\n",
    "df = pd.read_csv('./med-en-data.csv')\n",
    "df = df.astype(str)\n",
    "columns_to_drop = ['Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 0.2']\n",
    "df = df.drop(columns_to_drop, axis=1)\n",
    "print('Data loaded')\n",
    "\n",
    "# Filter rows where 'Quesition' or 'Answer' have more than 223 words\n",
    "# df = df[df['Answer'].apply(lambda x: len(x.split()) <= 256)]\n",
    "# df = df[df['Quesition'].apply(lambda x: len(x.split()) <= 256)]\n",
    "print('Data filtered')\n",
    "\n",
    "# Preprocess questions and answers\n",
    "df['Quesition'] = df['Quesition'].apply(preprocess_text)\n",
    "df['Answer'] = df['Answer'].apply(lambda x : preprocess_text(x , is_answer= True))\n",
    "print('Preprocessing done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3823abfa-b73b-4fd7-90cf-5f2d1077cccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check function applied\n"
     ]
    }
   ],
   "source": [
    "# Define the check function\n",
    "def check(row):\n",
    "    q = row['Quesition']\n",
    "    a = row['Answer']\n",
    "    # q_clean = re.sub(r'\\?$', '', q)  # Remove trailing question mark for comparison\n",
    "    if a.startswith(q):\n",
    "        a = a[len(q):].strip()\n",
    "    return q, a\n",
    "\n",
    "# Apply the check function\n",
    "df[['Quesition', 'Answer']] = df.apply(lambda row: pd.Series(check(row)), axis=1)\n",
    "print('Check function applied')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9784b255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1af75bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16209, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "907fc8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_with_special_tokens(text):\n",
    "#     text = preprocess_text(text)\n",
    "#     return f\"<start> {text} <end>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afafc926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Quesition'] = df['Quesition'].apply(lambda x : '<start> '+ x + ' <end>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e7b5a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Quesition    False\n",
       "Answer       False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d842300",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['Quesition'].values\n",
    "y = df['Answer'].values\n",
    "max_sequence_length = 256\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "600f7a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what cause oculopharyngeal muscular dystrophy ?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[222]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de337d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oculopharyngeal muscular dystrophy opmd is caused by mutation in the pabpn1 gene the pabpn1 gene provides instruction for making a protein that is active expressed throughout the body in cell the pabpn1 protein play an important role in processing molecule called messenger rna mrna which serve a genetic blueprint for making protein the protein act to protect the mrna from being broken down and allows it to move within the cell mutation in the pabpn1 gene that cause opmd result in a pabpn1 protein that form clump within muscle cell and hence they can not be broken down these clump are thought to impair the normal function of muscle cell and eventually cause cell to die the progressive loss of muscle cell most likely cause the muscle weakness seen in people with opmd it is not known why abnormal pabpn1 protein seem to affect muscle cell in only certain part of the body'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[222]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90af9da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer(num_words=None, oov_token='<OOV>' , filters='!\"#$%&()*+-./;<=>@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(df['Quesition'].tolist() + df['Answer'].tolist())\n",
    "# Manually add start and end tokens to the word index\n",
    "start_token = '<start>'\n",
    "end_token = '<end>'\n",
    "start_token_index = len(tokenizer.word_index) + 1\n",
    "end_token_index = len(tokenizer.word_index) + 2\n",
    "tokenizer.word_index[start_token] = start_token_index\n",
    "tokenizer.word_index[end_token] = end_token_index\n",
    "\n",
    "# Update the index_word dictionary to reflect these changes\n",
    "tokenizer.index_word[start_token_index] = start_token\n",
    "tokenizer.index_word[end_token_index] = end_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "892e57ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = tokenizer.texts_to_sequences(x)\n",
    "# input_sequences = [[start_token_index] + list(seq) for seq in input_sequences]\n",
    "input_sequences_padded = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "target_sequences = tokenizer.texts_to_sequences(y)\n",
    "target_sequences_padded = pad_sequences(target_sequences, maxlen=max_sequence_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a72f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index for the <start> and <end> tokens\n",
    "start_token_index = tokenizer.word_index['<start>']\n",
    "end_token_index = tokenizer.word_index['<end>']\n",
    "\n",
    "# Initialize decoder input and target data with the correct shape\n",
    "decoder_input_data = np.zeros((target_sequences_padded.shape[0], max_sequence_length), dtype=np.int64)\n",
    "decoder_target_data = np.zeros((target_sequences_padded.shape[0], max_sequence_length), dtype=np.int64)\n",
    "\n",
    "# Process each sequence\n",
    "for i in range(len(target_sequences_padded)):\n",
    "    # Initialize decoder input with <start> token\n",
    "    decoder_input_data[i, 0] = start_token_index\n",
    "    \n",
    "    # Fill decoder_input_data with the shifted target sequences\n",
    "    for t in range(1, max_sequence_length):\n",
    "        if t <= len(target_sequences_padded[i]):\n",
    "            if t - 1 < len(target_sequences_padded[i]) and target_sequences_padded[i, t - 1] != 0:\n",
    "                decoder_input_data[i, t] = target_sequences_padded[i, t - 1]\n",
    "            else:\n",
    "                decoder_input_data[i, t] = 0  # Padding\n",
    "\n",
    "    # Fill decoder_target_data with the target sequences and add the <end> token\n",
    "    for t in range(max_sequence_length - 1):\n",
    "        if t < len(target_sequences_padded[i]) and target_sequences_padded[i, t] != 0:\n",
    "            decoder_target_data[i, t] = target_sequences_padded[i, t]\n",
    "        else:\n",
    "            break\n",
    "    if t + 1 < max_sequence_length:\n",
    "        decoder_target_data[i, t] = end_token_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa4f03ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16209, 256), (16209, 256), (16209, 256))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_data.shape , decoder_input_data.shape , input_sequences_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9edbda44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what are the treatment for what i need to know about kidney failure and how it treated ? <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([input_sequences_padded[100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a88c22c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> if you have kidney failure learn about the treatment and think about which one best fit you talk with people who are on hemodialysis or peritoneal dialysis ask what is good and bad about each treatment if you make a choice and find you dont like it talk with your doctor about trying something else ask your doctor about the transplant waiting list and the medicine needed after a transplant talk with people who have had kidney transplant and ask how it ha changed their life if you plan to keep working think about which treatment choice would make working easier if spending time with family and friend mean a lot to you ask which treatment give you the most free time find out which treatment will give you the best chance to be healthy and live longer if you are thinking about conservative management you may wish to speak with your family friend doctor or mental health counselor a you decide you can take control of your care by talking with your doctor you may need time to get used to your new treatment kidney failure can make your life harder treatment can help improve your life <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([decoder_input_data[100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb1471a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['if you have kidney failure learn about the treatment and think about which one best fit you talk with people who are on hemodialysis or peritoneal dialysis ask what is good and bad about each treatment if you make a choice and find you dont like it talk with your doctor about trying something else ask your doctor about the transplant waiting list and the medicine needed after a transplant talk with people who have had kidney transplant and ask how it ha changed their life if you plan to keep working think about which treatment choice would make working easier if spending time with family and friend mean a lot to you ask which treatment give you the most free time find out which treatment will give you the best chance to be healthy and live longer if you are thinking about conservative management you may wish to speak with your family friend doctor or mental health counselor a you decide you can take control of your care by talking with your doctor you may need time to get used to your new treatment kidney failure can make your life harder treatment can help improve your life <end> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([decoder_target_data[100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c51d219",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_q, x_test_q1, y_train_a, y_test_a1 = train_test_split(input_sequences_padded, decoder_target_data, test_size=0.15, random_state=16)\n",
    "x_test_q, x_val_q, y_test_a, y_val_a = train_test_split(x_test_q1, y_test_a1, test_size=0.5, random_state=16)\n",
    "\n",
    "\n",
    "x_train_m , x_test_m1 = train_test_split(decoder_input_data , test_size=0.15,random_state=16)\n",
    "x_test_m , x_val_m = train_test_split(x_test_m1 , test_size=0.5 , random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98adeb09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13777, 256), (13777, 256), (13777, 256))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_m.shape , x_train_q.shape , y_train_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc713d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['qthree day after starting a new drug for malaria prophylaxis a 19 year old college student come to the physician because of dark colored urine and fatigue he ha not had any fever dysuria or abdominal pain he ha no history of serious illness physical examination show scleral icterus laboratory study show a hemoglobin of 97 g dl and serum lactate dehydrogenase of 234 u l peripheral blood smear show poikilocytes with bite shaped irregularity which of the following drug ha the patient most likely been taking ? a pyrimethamine b primaquine c wouldapsone would ivermectin e wouldoxycycline <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>'],\n",
       " ['<start> the answer is b primaquine <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>'],\n",
       " ['the answer is b primaquine <end> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([x_val_q[1]]) , tokenizer.sequences_to_texts([x_val_m[1]]) , tokenizer.sequences_to_texts([y_val_a[1]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0e9ecc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 31989)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_length , len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e8af107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "from keras import layers\n",
    "\n",
    "embed_dim = 128\n",
    "latent_dim = 400\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "enc_inp = Input(shape=(max_sequence_length,))\n",
    "dec_inp = Input(shape=(max_sequence_length,))\n",
    "\n",
    "embed = Embedding(vocab_size, output_dim=embed_dim, trainable=True)\n",
    "\n",
    "enc_embed = embed(enc_inp)\n",
    "enc_gru = GRU(latent_dim, return_sequences=True, return_state=True, dropout=0.24)\n",
    "enc_op, enc_state = enc_gru(enc_embed)\n",
    "enc_op = LayerNormalization()(enc_op)\n",
    "\n",
    "# enc_gru = GRU(latent_dim, return_sequences=True, return_state=True, dropout=0.3)\n",
    "# enc_op1, enc_state1 = enc_gru(enc_op)\n",
    "# enc_op1 = LayerNormalization()(enc_op1)\n",
    "\n",
    "dec_embed = embed(dec_inp)\n",
    "dec_gru = GRU(latent_dim, return_sequences=True, return_state=True, dropout=0.25)\n",
    "dec_op, _ = dec_gru(dec_embed, initial_state=enc_state)\n",
    "dec_op = LayerNormalization()(dec_op)\n",
    "\n",
    "dec_gru = GRU(latent_dim, return_sequences=True, return_state=True, dropout=0.3 )\n",
    "dec_op1, _ = dec_gru(dec_op, initial_state=enc_state)\n",
    "dec_op1 = LayerNormalization()(dec_op1)\n",
    "\n",
    "# dec_gru = GRU(latent_dim, return_sequences=True, return_state=True, dropout=0.3)\n",
    "# dec_op2, _ = dec_gru(dec_op1, initial_state=enc_state)\n",
    "# dec_op2 = LayerNormalization()(dec_op2)\n",
    "\n",
    "attention = AdditiveAttention()\n",
    "context_vector, attention_weights = attention([dec_op1, enc_op], return_attention_scores=True)\n",
    "context_add = Concatenate()([context_vector, dec_op1])\n",
    "\n",
    "# dense1 = TimeDistributed(Dense(embed_dim, activation='relu'))\n",
    "# dense_op1 = dense1(context_add)\n",
    "# dense_op1 = Dropout(0.3)(dense_op1)\n",
    "\n",
    "dense = TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
    "dense_op = dense(context_add)\n",
    "\n",
    "model = Model([enc_inp, dec_inp], dense_op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "861a8c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.optimizers import RMSprop\n",
    "model.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8e7cf70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 256, 128)     4094592     ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " gru (GRU)                      [(None, 256, 400),   636000      ['embedding[0][0]']              \n",
      "                                 (None, 400)]                                                     \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                    [(None, 256, 400),   636000      ['embedding[1][0]',              \n",
      "                                 (None, 400)]                     'gru[0][1]']                    \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 256, 400)    800         ['gru_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " gru_2 (GRU)                    [(None, 256, 400),   962400      ['layer_normalization_1[0][0]',  \n",
      "                                 (None, 400)]                     'gru[0][1]']                    \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 256, 400)    800         ['gru_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 256, 400)    800         ['gru[0][0]']                    \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " additive_attention (AdditiveAt  ((None, 256, 400),  400         ['layer_normalization_2[0][0]',  \n",
      " tention)                        (None, 256, 256))                'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 256, 800)     0           ['additive_attention[0][0]',     \n",
      "                                                                  'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, 256, 31989)  25623189    ['concatenate[0][0]']            \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 31,954,981\n",
      "Trainable params: 31,954,981\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c2565454",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint('./check_model_position_11.h5',verbose=1 , save_best_only=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, min_lr=0.00001),\n",
    "    CSVLogger('data_train.csv'),\n",
    "    TensorBoard(),\n",
    "    EarlyStopping(monitor='val_loss' ,patience=10 , restore_best_weights=False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eb617c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6889/6889 [==============================] - ETA: 0s - loss: 1.4862\n",
      "Epoch 1: val_loss improved from inf to 1.28673, saving model to .\\check_model_position_11.h5\n",
      "6889/6889 [==============================] - 1154s 167ms/step - loss: 1.4862 - val_loss: 1.2867 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "6889/6889 [==============================] - ETA: 0s - loss: 1.1920\n",
      "Epoch 2: val_loss improved from 1.28673 to 1.20597, saving model to .\\check_model_position_11.h5\n",
      "6889/6889 [==============================] - 1151s 167ms/step - loss: 1.1920 - val_loss: 1.2060 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "6889/6889 [==============================] - ETA: 0s - loss: 1.0673\n",
      "Epoch 3: val_loss improved from 1.20597 to 1.15855, saving model to .\\check_model_position_11.h5\n",
      "6889/6889 [==============================] - 1152s 167ms/step - loss: 1.0673 - val_loss: 1.1585 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "6889/6889 [==============================] - ETA: 0s - loss: 0.9683\n",
      "Epoch 4: val_loss improved from 1.15855 to 1.12799, saving model to .\\check_model_position_11.h5\n",
      "6889/6889 [==============================] - 1150s 167ms/step - loss: 0.9683 - val_loss: 1.1280 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "6889/6889 [==============================] - ETA: 0s - loss: 0.8914\n",
      "Epoch 5: val_loss improved from 1.12799 to 1.12254, saving model to .\\check_model_position_11.h5\n",
      "6889/6889 [==============================] - 1150s 167ms/step - loss: 0.8914 - val_loss: 1.1225 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "6889/6889 [==============================] - ETA: 0s - loss: 0.8366\n",
      "Epoch 6: val_loss improved from 1.12254 to 1.12183, saving model to .\\check_model_position_11.h5\n",
      "6889/6889 [==============================] - 1149s 167ms/step - loss: 0.8366 - val_loss: 1.1218 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "6889/6889 [==============================] - ETA: 0s - loss: 0.7944\n",
      "Epoch 7: val_loss did not improve from 1.12183\n",
      "6889/6889 [==============================] - 1149s 167ms/step - loss: 0.7944 - val_loss: 1.1292 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "6889/6889 [==============================] - ETA: 0s - loss: 0.7624\n",
      "Epoch 8: val_loss did not improve from 1.12183\n",
      "6889/6889 [==============================] - 1148s 167ms/step - loss: 0.7624 - val_loss: 1.1277 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "6889/6889 [==============================] - ETA: 0s - loss: 0.7373\n",
      "Epoch 9: val_loss did not improve from 1.12183\n",
      "6889/6889 [==============================] - 1149s 167ms/step - loss: 0.7373 - val_loss: 1.1326 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "6889/6889 [==============================] - ETA: 0s - loss: 0.7170\n",
      "Epoch 10: val_loss did not improve from 1.12183\n",
      "6889/6889 [==============================] - 1148s 167ms/step - loss: 0.7170 - val_loss: 1.1398 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "6889/6889 [==============================] - ETA: 0s - loss: 0.6185\n",
      "Epoch 11: val_loss improved from 1.12183 to 1.12064, saving model to .\\check_model_position_11.h5\n",
      "6889/6889 [==============================] - 1146s 166ms/step - loss: 0.6185 - val_loss: 1.1206 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "6889/6889 [==============================] - ETA: 0s - loss: 0.5855\n",
      "Epoch 12: val_loss did not improve from 1.12064\n",
      "6889/6889 [==============================] - 1143s 166ms/step - loss: 0.5855 - val_loss: 1.1260 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "6889/6889 [==============================] - ETA: 0s - loss: 0.5673\n",
      "Epoch 13: val_loss did not improve from 1.12064\n",
      "6889/6889 [==============================] - 1187s 172ms/step - loss: 0.5673 - val_loss: 1.1305 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "6889/6889 [==============================] - ETA: 0s - loss: 0.5543\n",
      "Epoch 14: val_loss did not improve from 1.12064\n",
      "6889/6889 [==============================] - 1286s 187ms/step - loss: 0.5543 - val_loss: 1.1348 - lr: 1.0000e-04\n",
      "Epoch 15/50\n",
      "6889/6889 [==============================] - ETA: 0s - loss: 0.5442\n",
      "Epoch 15: val_loss did not improve from 1.12064\n",
      "6889/6889 [==============================] - 1135s 165ms/step - loss: 0.5442 - val_loss: 1.1408 - lr: 1.0000e-04\n",
      "Epoch 16/50\n",
      "6889/6889 [==============================] - ETA: 0s - loss: 0.5287\n",
      "Epoch 16: val_loss did not improve from 1.12064\n",
      "6889/6889 [==============================] - 1133s 164ms/step - loss: 0.5287 - val_loss: 1.1430 - lr: 1.0000e-05\n",
      "Epoch 17/50\n",
      "6889/6889 [==============================] - ETA: 0s - loss: 0.5262\n",
      "Epoch 17: val_loss did not improve from 1.12064\n",
      "6889/6889 [==============================] - 1132s 164ms/step - loss: 0.5262 - val_loss: 1.1434 - lr: 1.0000e-05\n",
      "Epoch 18/50\n",
      "6889/6889 [==============================] - ETA: 0s - loss: 0.5251\n",
      "Epoch 18: val_loss did not improve from 1.12064\n",
      "6889/6889 [==============================] - 1135s 165ms/step - loss: 0.5251 - val_loss: 1.1443 - lr: 1.0000e-05\n",
      "Epoch 19/50\n",
      "6889/6889 [==============================] - ETA: 0s - loss: 0.5240\n",
      "Epoch 19: val_loss did not improve from 1.12064\n",
      "6889/6889 [==============================] - 1129s 164ms/step - loss: 0.5240 - val_loss: 1.1450 - lr: 1.0000e-05\n",
      "Epoch 20/50\n",
      "6889/6889 [==============================] - ETA: 0s - loss: 0.5228\n",
      "Epoch 20: val_loss did not improve from 1.12064\n",
      "6889/6889 [==============================] - 1131s 164ms/step - loss: 0.5228 - val_loss: 1.1459 - lr: 1.0000e-05\n",
      "Epoch 21/50\n",
      "6889/6889 [==============================] - ETA: 0s - loss: 0.5215\n",
      "Epoch 21: val_loss did not improve from 1.12064\n",
      "6889/6889 [==============================] - 1131s 164ms/step - loss: 0.5215 - val_loss: 1.1468 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x211907fc0a0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    [x_train_q , x_train_m],\n",
    "    y_train_a,\n",
    "    validation_data=([x_val_q , x_val_m] , y_val_a),\n",
    "    epochs=50,\n",
    "    shuffle = True,\n",
    "    callbacks = callbacks,\n",
    "    batch_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec9de013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(\n",
    "#     [x_train_q[5000:], x_train_m[5000:]],\n",
    "#     y_train_a[5000:],\n",
    "#     validation_data=([x_val_q , x_val_m] , y_val_a),\n",
    "#     epochs=100,\n",
    "#     shuffle = True,\n",
    "#     callbacks = callbacks,\n",
    "#     batch_size=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4413020f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "\n",
    "# model111 = load_model(\"C:/Users/Ahmed/Downloads/dataset/check_model_position_11.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "48fcd657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "608/608 [==============================] - 28s 45ms/step - loss: 1.1940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.193995475769043"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([x_test_q , x_test_m] , y_test_a , batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a43b8b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_single_text(text, tokenizer, max_sequence_length):\n",
    "    text = preprocess_text(text)\n",
    "    # text = f\"<start> {text} <end>\"\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    # sequence = [[start_token_index] + list(seq) + [end_token_index] for seq in sequence]\n",
    "    print(tokenizer.sequences_to_texts(sequence))\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_sequence_length, padding='post')\n",
    "    return padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9f66da8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_answer(question, model, tokenizer, max_sequence_length):\n",
    "    question_seq = preprocess_single_text(question, tokenizer, max_sequence_length)\n",
    "    \n",
    "    # start_token = np.array([[tokenizer.word_index['<start>']]])\n",
    "    answer_seq = np.ones((1, max_sequence_length))\n",
    "    answer_seq[0, 0] = tokenizer.word_index['<start>']\n",
    "    reverse_word_index = {index: word for word, index in tokenizer.word_index.items()}\n",
    "\n",
    "\n",
    "    for i in range(1, 64):\n",
    "        output_tokens = model.predict([question_seq, answer_seq], verbose=0 , batch_size=2)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, i-1, :])\n",
    "        answer_seq[0, i] = sampled_token_index\n",
    "\n",
    "        if sampled_token_index == tokenizer.word_index['<end>']:\n",
    "            break\n",
    "\n",
    "    answer_tokens = [reverse_word_index[int(idx)] for idx in answer_seq[0] if idx > 0]\n",
    "    \n",
    "    if '<start>' in answer_tokens:\n",
    "        answer_tokens.remove('<start>')\n",
    "    if '<end>' in answer_tokens:\n",
    "        answer_tokens.remove('<end>')\n",
    "    \n",
    "    \n",
    "    answer_tokens = [token for token in answer_tokens if token != '<OOV>']\n",
    "    answer_tokens = [token for token in answer_tokens if token != '<start>']\n",
    "\n",
    "    \n",
    "\n",
    "    answer = ' '.join(answer_tokens)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f78f215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"Q:A 2 year old boy is brought to the physician because of fatigue and yellow discoloration of his skin for 2 days. One week ago, he had a 3 day course of low grade fever and runny nose. As a newborn, he underwent a 5 day course of phototherapy for neonatal jaundice. His vital signs are within normal limits. Examination shows jaundice of the skin and conjunctivae. The spleen tip is palpated 3 cm below the left costal margin. His hemoglobin is 9.8 g dl and mean corpuscular hemoglobin concentration is 38 Hb cell. A Coombs test is negative. A peripheral blood smear is shown. This patient is at greatest risk for which of the following complications?? 'A': 'Malaria', 'B': 'Acute chest syndrome', 'C': 'Osteomyelitis', 'D': 'Acute myelogenous leukemia', 'E': 'Cholecystitis' ,\"\n",
    "# predicted_answer = predict_answer(question, model, tokenizer, max_sequence_length)\n",
    "# print(\"Question:\", question)\n",
    "# print(\"Answer:\", predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fe72fdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = df['Quesition'].values\n",
    "yy = df['Answer'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "680ef39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('is aicardi goutieres syndrome inherited ?',\n",
       " 'aicardi goutieres syndrome can have different inheritance pattern in most case it is inherited in an autosomal recessive pattern which mean both copy of the gene in each cell have mutation the parent of an individual with an autosomal recessive condition each carry one copy of the mutated gene but they typically do not show sign and symptom of the condition rarely this condition is inherited in an autosomal dominant pattern which mean one copy of the altered gene in each cell is sufficient to cause the disorder these case result from new mutation in the gene and occur in people with no history of the disorder in their family')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx[1] , yy[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b8a3819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is aicardi goutieres syndrome inherited ?']\n",
      "Question: is aicardi goutieres syndrome inherited ?\n",
      "Answer: aicardi goutieres syndrome is inherited in an autosomal dominant pattern which mean one copy of the altered gene in each cell is sufficient to cause the disorder\n"
     ]
    }
   ],
   "source": [
    "question = xx[1]\n",
    "predicted_answer = predict_answer(question, model, tokenizer, max_sequence_length)\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d9f2361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how are you']\n",
      "Question:  how are you\n",
      "Answer:  i am specialized in medical advice please ask health related question\n"
     ]
    }
   ],
   "source": [
    "question = \"how are you\"\n",
    "predicted_answer = predict_answer(question, model, tokenizer, max_sequence_length)\n",
    "print(\"Question: \", question)\n",
    "print(\"Answer: \", predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f0b604c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello how are you']\n",
      "Question:  hello how are you\n",
      "Answer:  hello how i can i help you ?\n"
     ]
    }
   ],
   "source": [
    "question = \"hello how are you\"\n",
    "predicted_answer = predict_answer(question, model, tokenizer, max_sequence_length)\n",
    "print(\"Question: \", question)\n",
    "print(\"Answer: \", predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "95397396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what are the sign of the flu ?']\n",
      "Question:  what are the sign of the flu ?\n",
      "Answer:  the answer is a small piece of material that surround the lung and the large intestine branch away from the lung to the rest of the body the study of the small intestine that surround the lung and the large intestine are small bean shaped organ and each of the small intestine and large intestine are small bean shaped structure that help break\n"
     ]
    }
   ],
   "source": [
    "question = \"what are the sign of the flu ?\"\n",
    "predicted_answer = predict_answer(question, model, tokenizer, max_sequence_length)\n",
    "print(\"Question: \", question)\n",
    "print(\"Answer: \", predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "abfe354b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what is love ?']\n",
      "Question:  what is love ?\n",
      "Answer:  the answer is a mental health disorder characterised by a harmful substance called a plaque in the inner ear which is a part of the brain that control the movement of the arm and leg the most common symptom of a person with a spinal cancer may be a sign of the disease a well a those of the same family a well\n"
     ]
    }
   ],
   "source": [
    "question = \"what is love ?\"\n",
    "predicted_answer = predict_answer(question, model, tokenizer, max_sequence_length)\n",
    "print(\"Question: \", question)\n",
    "print(\"Answer: \", predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "53e80442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what going on with you ?']\n",
      "Question:  what going on with you?\n",
      "Answer:  i am zkzk bot your personal health care ai assistant how are you feeling today\n"
     ]
    }
   ],
   "source": [
    "question = \"what going on with you?\"\n",
    "predicted_answer = predict_answer(question, model, tokenizer, max_sequence_length)\n",
    "print(\"Question: \", question)\n",
    "print(\"Answer: \", predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d9d6d266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what is cold ?']\n",
      "Question:  what's cold?\n",
      "Answer:  vre is a rare condition that cause the symptom of mental state and the disease the symptom of this condition typically begin in mid adulthood the underlying cause of the condition is unknown there is no evidence of the various type of treatment the underlying condition is available\n"
     ]
    }
   ],
   "source": [
    "question = \"what's cold?\"\n",
    "predicted_answer = predict_answer(question, model, tokenizer, max_sequence_length)\n",
    "print(\"Question: \", question)\n",
    "print(\"Answer: \", predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "675b11e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am i feel happy ?']\n",
      "Question:  am I feel happy?\n",
      "Answer:  how might i be treated ?\n"
     ]
    }
   ],
   "source": [
    "question = \"am I feel happy?\"\n",
    "predicted_answer = predict_answer(question, model, tokenizer, max_sequence_length)\n",
    "print(\"Question: \", question)\n",
    "print(\"Answer: \", predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2d1029e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('try_the_best_51.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "70303663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model1 = load_model('C:/Users/Ahmed/Downloads/dataset/try_the_best_51.h5')\n",
    "\n",
    "# Freeze the layers of the pre-trained model\n",
    "for layer in model1.layers[:int(0.80 * len(model1.layers))]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile the model\n",
    "model1.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "66e8fb0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 256, 31989)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a32353a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am i feel happy ?']\n",
      "Question:  am I feel happy?\n",
      "Answer:  how might i be treated ?\n"
     ]
    }
   ],
   "source": [
    "question = \"am I feel happy?\"\n",
    "predicted_answer = predict_answer(question, model, tokenizer, max_sequence_length)\n",
    "print(\"Question: \", question)\n",
    "print(\"Answer: \", predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4cca0765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31988"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c660268c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "608/608 [==============================] - 28s 46ms/step - loss: 0.9723\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9723271727561951"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate([x_test_q , x_test_m] , y_test_a , batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "de2c1480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# model1 = load_model('C:/Users/Ahmed/Downloads/dataset/try_the_best_3.h5')\n",
    "\n",
    "# # Freeze the layers of the pre-trained model\n",
    "# for layer in model1.layers[:int(0.7 * len(model1.layers))]:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# # Recompile the model\n",
    "model1.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dc5cb55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "608/608 [==============================] - ETA: 0s - loss: 1.0756\n",
      "Epoch 1: val_loss did not improve from 1.12064\n",
      "608/608 [==============================] - 78s 125ms/step - loss: 1.0756 - val_loss: 1.2599 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "608/608 [==============================] - ETA: 0s - loss: 0.8515\n",
      "Epoch 2: val_loss did not improve from 1.12064\n",
      "608/608 [==============================] - 76s 124ms/step - loss: 0.8515 - val_loss: 1.2536 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "608/608 [==============================] - ETA: 0s - loss: 0.7030\n",
      "Epoch 3: val_loss did not improve from 1.12064\n",
      "608/608 [==============================] - 76s 124ms/step - loss: 0.7030 - val_loss: 1.2697 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "608/608 [==============================] - ETA: 0s - loss: 0.6125\n",
      "Epoch 4: val_loss did not improve from 1.12064\n",
      "608/608 [==============================] - 76s 125ms/step - loss: 0.6125 - val_loss: 1.2927 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "608/608 [==============================] - ETA: 0s - loss: 0.5671\n",
      "Epoch 5: val_loss did not improve from 1.12064\n",
      "608/608 [==============================] - 76s 125ms/step - loss: 0.5671 - val_loss: 1.3129 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "608/608 [==============================] - ETA: 0s - loss: 0.5338\n",
      "Epoch 6: val_loss did not improve from 1.12064\n",
      "608/608 [==============================] - 76s 125ms/step - loss: 0.5338 - val_loss: 1.3334 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "608/608 [==============================] - ETA: 0s - loss: 0.4841\n",
      "Epoch 7: val_loss did not improve from 1.12064\n",
      "608/608 [==============================] - 76s 125ms/step - loss: 0.4841 - val_loss: 1.3299 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "608/608 [==============================] - ETA: 0s - loss: 0.4751\n",
      "Epoch 8: val_loss did not improve from 1.12064\n",
      "608/608 [==============================] - 76s 125ms/step - loss: 0.4751 - val_loss: 1.3312 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "608/608 [==============================] - ETA: 0s - loss: 0.4673\n",
      "Epoch 9: val_loss did not improve from 1.12064\n",
      "608/608 [==============================] - 76s 125ms/step - loss: 0.4673 - val_loss: 1.3336 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "608/608 [==============================] - ETA: 0s - loss: 0.4632\n",
      "Epoch 10: val_loss did not improve from 1.12064\n",
      "608/608 [==============================] - 76s 125ms/step - loss: 0.4632 - val_loss: 1.3362 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "608/608 [==============================] - ETA: 0s - loss: 0.4555\n",
      "Epoch 11: val_loss did not improve from 1.12064\n",
      "608/608 [==============================] - 76s 125ms/step - loss: 0.4555 - val_loss: 1.3364 - lr: 1.0000e-05\n",
      "Epoch 12/50\n",
      "608/608 [==============================] - ETA: 0s - loss: 0.4545\n",
      "Epoch 12: val_loss did not improve from 1.12064\n",
      "608/608 [==============================] - 76s 125ms/step - loss: 0.4545 - val_loss: 1.3365 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2129ff50b20>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(\n",
    "    [x_test_q , x_test_m] , y_test_a,\n",
    "    epochs = 50,\n",
    "    batch_size=2,\n",
    "    validation_data=([x_val_q , x_val_m] , y_val_a),\n",
    "    callbacks = callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7467576c-6972-4531-be50-a85a24c58324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what are the sign of the flu ?']\n",
      "Question:  what are the sign of the flu ?\n",
      "Answer:  the most common cause of the condition is not known however some people with a blockage of the have no symptom do not have symptom associated with the condition however some people with a blockage of the pancreas do not have symptom associated with the condition however some people with a blockage of the pancreas may not become susceptible to a respiratory infection\n"
     ]
    }
   ],
   "source": [
    "question = \"what are the sign of the flu ?\"\n",
    "predicted_answer = predict_answer(question, model1, tokenizer, max_sequence_length)\n",
    "print(\"Question: \", question)\n",
    "print(\"Answer: \", predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c9839407-a188-4960-a68c-819f3cef1252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello how are you']\n",
      "Question:  hello , how are you\n",
      "Answer:  hello how i i can help you ?\n"
     ]
    }
   ],
   "source": [
    "question = \"hello , how are you\"\n",
    "predicted_answer = predict_answer(question, model1, tokenizer, max_sequence_length)\n",
    "print(\"Question: \", question)\n",
    "print(\"Answer: \", predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ece79d5f-1a19-41f6-bfa9-aee3cb22007f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello']\n",
      "Question:  hello\n",
      "Answer:  hi how is i am here\n"
     ]
    }
   ],
   "source": [
    "question = \"hello\"\n",
    "predicted_answer = predict_answer(question, model1, tokenizer, max_sequence_length)\n",
    "print(\"Question: \", question)\n",
    "print(\"Answer: \", predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "554a49b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a 2 year old boy is brought to the physician because of fatigue and yellow discoloration of his skin for 2 day one week ago he had a 3 day course of low grade fever and runny nose a a newborn he underwent a 5 day course of phototherapy for neonatal jaundice his vital sign are within normal limit examination show jaundice of the skin and conjunctiva the spleen tip is palpated 3 cm below the left costal margin his hemoglobin is 98 g dl and mean corpuscular hemoglobin concentration is 38 hb cell a coombs test is negative a peripheral blood smear is shown this patient is at greatest risk for which of the following complication ? a malaria b acute chest syndrome c osteomyelitis would acute myelogenous leukemia e cholecystitis']\n",
      "Question: a 2 year old boy is brought to the physician because of fatigue and yellow discoloration of his skin for 2 day one week ago he had a 3 day course of low grade fever and runny nose a a newborn he underwent a 5 day course of phototherapy for neonatal jaundice his vital sign are within normal limit examination show jaundice of the skin and conjunctiva the spleen tip is palpated 3 cm below the left costal margin his hemoglobin is 98 g dl and mean corpuscular hemoglobin concentration is 38 hb cell a coombs test is negative a peripheral blood smear is shown this patient is at greatest risk for which of the following complication ? a malaria b acute chest syndrome c osteomyelitis would acute myelogenous leukemia e cholecystitis\n",
      "Answer: the answer is e supportive\n"
     ]
    }
   ],
   "source": [
    "question = xx[0]\n",
    "predicted_answer = predict_answer(question, model1, tokenizer, max_sequence_length)\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "94c8785f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i feel bad today']\n",
      "Question: i feel bad today\n",
      "Answer: i am here for you with your query i can help you ?\n"
     ]
    }
   ],
   "source": [
    "question = 'i feel bad today'\n",
    "predicted_answer = predict_answer(question, model1, tokenizer, max_sequence_length)\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "511b53d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i have flu what should i do to reveal']\n",
      "Question: i have flu what should I do  to reveal\n",
      "Answer: i can not see if you are experiencing a eating a eating a balanced diet when you are a very severe with a healthy diet\n"
     ]
    }
   ],
   "source": [
    "question = 'i have flu what should I do  to reveal'\n",
    "predicted_answer = predict_answer(question, model1, tokenizer, max_sequence_length)\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043ba1c7-31df-4279-87bf-802b5a47f76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('lstm_2ly_tuned_4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617ba275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('tokenizer.pkl', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle)\n",
    "\n",
    "# loading\n",
    "with open('tokenizer.pkl', 'rb') as handle:\n",
    "    tokenizer_1 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83014141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "model1 = load_model('C:/Users/Ahmed/Downloads/dataset/check_model_position_11.h5')\n",
    "# load token\n",
    "with open('tokenizer.pkl', 'rb') as handle:\n",
    "    tokenizer_1 = pickle.load(handle)\n",
    "# load prerpocess functoion\n",
    "def preprocess_text(txt , is_answer = False):\n",
    "    txt = txt.lower()\n",
    "    contractions = {\n",
    "        \"i'm\": \"i am\", \"he's\": \"he is\", \"she's\": \"she is\", \"that's\": \"that is\",\n",
    "        \"what's\": \"what is\", \"where's\": \"where is\", \"'ll\": \" will\", \"'ve\": \" have\",\n",
    "        \"'re\": \" are\", \"'d\": \" would\", \"won't\": \"will not\", \"can't\": \"can not\",\n",
    "        \"wanna\": \"want to\" , \"q:a\": \"q: a\" , \"healthcaremagic.comi\" :\"healthcaremagic.com i\"\n",
    "    }\n",
    "    for contraction, replacement in contractions.items():\n",
    "        txt = re.sub(contraction, replacement, txt)\n",
    "    \n",
    "    txt = re.sub(r'[^\\w\\s\\?]', '', txt)\n",
    "    txt = re.sub(r'[^\\w\\s\\?]', '', txt)\n",
    "    if not is_answer:\n",
    "        txt = re.sub(r\"q \",'', txt)\n",
    "        txt = re.sub(r'\\?{2,}','?' , txt)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(txt)\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Join tokens back to a single string\n",
    "    processed_text = ' '.join(tokens)\n",
    "\n",
    "    # Modify multiple-choice answers to include the format \"the answer is x: y\"\n",
    "    if is_answer:\n",
    "        match = re.match(r'([a-e]) (.*)', processed_text)\n",
    "        if match:\n",
    "            processed_text = f\"the answer is {match.group(1)} {match.group(2)}\"\n",
    "    if processed_text.endswith(','):\n",
    "        processed_text = processed_text[:-2]\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "#  load the prediction function\n",
    "def preprocess_single_text(text, tokenizer, max_sequence_length):\n",
    "    text = preprocess_text(text)\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_sequence_length, padding='post')\n",
    "    return padded_sequence\n",
    "\n",
    "def predict_answer(question, model, tokenizer, max_sequence_length):\n",
    "    question_seq = preprocess_single_text(question, tokenizer, max_sequence_length)\n",
    "    answer_seq = np.ones((1, max_sequence_length))\n",
    "    answer_seq[0, 0] = tokenizer.word_index['<start>']\n",
    "    reverse_word_index = {index: word for word, index in tokenizer.word_index.items()}\n",
    "    for i in range(1, 64):\n",
    "        output_tokens = model.predict([question_seq, answer_seq], verbose=0 , batch_size=2)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, i-1, :])\n",
    "        answer_seq[0, i] = sampled_token_index\n",
    "\n",
    "        if sampled_token_index == tokenizer.word_index['<end>']:\n",
    "            break\n",
    "\n",
    "    answer_tokens = [reverse_word_index[int(idx)] for idx in answer_seq[0] if idx > 0]\n",
    "    \n",
    "    if '<start>' in answer_tokens:\n",
    "        answer_tokens.remove('<start>')\n",
    "    if '<end>' in answer_tokens:\n",
    "        answer_tokens.remove('<end>')\n",
    "    answer_tokens = [token for token in answer_tokens if token != '<OOV>']\n",
    "    answer_tokens = [token for token in answer_tokens if token != '<start>']\n",
    "\n",
    "    answer = ' '.join(answer_tokens)\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71936c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer_1.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2782d2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'hello xkxk'\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c2aa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'how can I code in love'\n",
    "predicted_answer = predict_answer(question, model1, tokenizer_1, 224)\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d58b50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af08c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TransformerEncoder(layers.Layer):\n",
    "#     def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "#         super().__init__(**kwargs)\n",
    "#         self.embed_dim = embed_dim \n",
    "#         self.dense_dim = dense_dim \n",
    "#         self.num_heads = num_heads \n",
    "#         self.attention = MultiHeadAttention(\n",
    "#         num_heads=num_heads, key_dim=embed_dim)\n",
    "#         self.dense_proj = Sequential(\n",
    "#         [Dense(dense_dim, activation=\"relu\"),\n",
    "#         Dense(embed_dim),]\n",
    "#         )\n",
    "#         self.layernorm_1 = LayerNormalization()\n",
    "#         self.layernorm_2 = LayerNormalization()\n",
    "#     def call(self, inputs, mask=None): \n",
    "#         if mask is not None: \n",
    "#             mask = mask[:, tf.newaxis, :] \n",
    "#         attention_output = self.attention(\n",
    "#             inputs, inputs, attention_mask=mask)\n",
    "#         proj_input = self.layernorm_1(inputs + attention_output)\n",
    "#         proj_output = self.dense_proj(proj_input)\n",
    "#         return self.layernorm_2(proj_input + proj_output)\n",
    "#     def get_config(self): \n",
    "#         config = super().get_config()\n",
    "#         config.update({\n",
    "#         \"embed_dim\": self.embed_dim,\n",
    "#         \"num_heads\": self.num_heads,\n",
    "#         \"dense_dim\": self.dense_dim,\n",
    "#         })\n",
    "#         return config\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim \n",
    "        self.dense_dim = dense_dim \n",
    "        self.num_heads = num_heads \n",
    "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = Sequential([\n",
    "            Dense(dense_dim, activation=\"relu\"),\n",
    "            Dense(embed_dim)  # Match the input embedding dimension\n",
    "        ])\n",
    "        self.layernorm_1 = LayerNormalization()\n",
    "        self.layernorm_2 = LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None): \n",
    "        if mask is not None: \n",
    "            mask = mask[:, tf.newaxis, :] \n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self): \n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba4086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PositionalEmbedding(layers.Layer):\n",
    "#     def __init__(self, sequence_length, input_dim, output_dim, **kwargs): \n",
    "#         super().__init__(**kwargs)\n",
    "#         self.token_embeddings = layers.Embedding( \n",
    "#         input_dim=input_dim, output_dim=output_dim)\n",
    "#         self.position_embeddings = layers.Embedding(\n",
    "#         input_dim=sequence_length, output_dim=output_dim) \n",
    "#         self.sequence_length = sequence_length\n",
    "#         self.input_dim = input_dim\n",
    "#         self.output_dim = output_dim\n",
    "#     def call(self, inputs):\n",
    "#         length = tf.shape(inputs)[-1]\n",
    "#         positions = tf.range(start=0, limit=length, delta=1)\n",
    "#         embedded_tokens = self.token_embeddings(inputs)\n",
    "#         embedded_positions = self.position_embeddings(positions)\n",
    "#         return embedded_tokens + embedded_positions \n",
    "#     def compute_mask(self, inputs, mask=None): \n",
    "#         return tf.math.not_equal(inputs, 0) \n",
    "#     def get_config(self): \n",
    "#         config = super().get_config()\n",
    "#         config.update({\n",
    "#         \"output_dim\": self.output_dim,\n",
    "#         \"sequence_length\": self.sequence_length,\n",
    "#         \"input_dim\": self.input_dim,\n",
    "#         })\n",
    "#         return config\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs): \n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = Embedding(input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = Embedding(input_dim=sequence_length, output_dim=output_dim) \n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions \n",
    "\n",
    "    def compute_mask(self, inputs, mask=None): \n",
    "        return tf.math.not_equal(inputs, 0) \n",
    "\n",
    "    def get_config(self): \n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76a9b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TransformerDecoder(layers.Layer):\n",
    "#     def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "#         super().__init__(**kwargs)\n",
    "#         self.embed_dim = embed_dim\n",
    "#         self.dense_dim = dense_dim\n",
    "#         self.num_heads = num_heads\n",
    "#         self.attention_1 = layers.MultiHeadAttention(\n",
    "#         num_heads=num_heads, key_dim=embed_dim)\n",
    "#         self.attention_2 = layers.MultiHeadAttention(\n",
    "#         num_heads=num_heads, key_dim=embed_dim)\n",
    "#         self.dense_proj = Sequential(\n",
    "#             [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "#         layers.Dense(embed_dim),]\n",
    "#         )\n",
    "#         self.layernorm_1 = layers.LayerNormalization()\n",
    "#         self.layernorm_2 = layers.LayerNormalization()\n",
    "#         self.layernorm_3 = layers.LayerNormalization()\n",
    "#         self.supports_masking = True \n",
    "#     def get_config(self):\n",
    "#         config = super().get_config()\n",
    "#         config.update({\n",
    "#         \"embed_dim\": self.embed_dim,\n",
    "#         \"num_heads\": self.num_heads,\n",
    "#         \"dense_dim\": self.dense_dim,\n",
    "#         })\n",
    "#         return config\n",
    "#     def get_causal_attention_mask(self, inputs):\n",
    "#         input_shape = tf.shape(inputs)\n",
    "#         batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "#         i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "#         j = tf.range(sequence_length)\n",
    "#         mask = tf.cast(i >= j, dtype=\"int32\") \n",
    "#         mask = tf.reshape(mask, (1, input_shape[1], input_shape[1])) \n",
    "#         mult = tf.concat( \n",
    "#         [tf.expand_dims(batch_size, -1), \n",
    "#         tf.constant([1, 1], dtype=tf.int32)], axis=0) \n",
    "#         return tf.tile(mask, mult)\n",
    "    \n",
    "#     def call(self, inputs, encoder_outputs, mask=None):\n",
    "#         causal_mask = self.get_causal_attention_mask(inputs) \n",
    "#         if mask is not None: \n",
    "#             padding_mask = tf.cast( \n",
    "#                 mask[:, tf.newaxis, :], dtype=\"int32\") \n",
    "#             padding_mask = tf.minimum(padding_mask, causal_mask) \n",
    "#         attention_output_1 = self.attention_1(\n",
    "#             query=inputs,\n",
    "#             value=inputs,\n",
    "#             key=inputs,\n",
    "#             attention_mask=causal_mask\n",
    "#         ) \n",
    "#         attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "#         attention_output_2 = self.attention_2(\n",
    "#             query=attention_output_1,\n",
    "#             value=encoder_outputs,\n",
    "#             key=encoder_outputs,\n",
    "#             attention_mask=padding_mask, \n",
    "#         )\n",
    "#         attention_output_2 = self.layernorm_2(\n",
    "#             attention_output_1 + attention_output_2\n",
    "#         )\n",
    "#         proj_output = self.dense_proj(attention_output_2)\n",
    "#         return self.layernorm_3(attention_output_2 + proj_output)\n",
    "\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attention_2 = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = Sequential([\n",
    "            Dense(dense_dim, activation=\"relu\"),\n",
    "            Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm_1 = LayerNormalization()\n",
    "        self.layernorm_2 = LayerNormalization()\n",
    "        self.layernorm_3 = LayerNormalization()\n",
    "        self.supports_masking = True \n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\") \n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1])) \n",
    "        mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], axis=0) \n",
    "        return tf.tile(mask, mult)\n",
    "    \n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs) \n",
    "        if mask is not None: \n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\") \n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask) \n",
    "        attention_output_1 = self.attention_1(query=inputs, value=inputs, key=inputs, attention_mask=causal_mask) \n",
    "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "        attention_output_2 = self.attention_2(query=attention_output_1, value=encoder_outputs, key=encoder_outputs, attention_mask=padding_mask)\n",
    "        attention_output_2 = self.layernorm_2(attention_output_1 + attention_output_2)\n",
    "        proj_output = self.dense_proj(attention_output_2)\n",
    "        return self.layernorm_3(attention_output_2 + proj_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef319e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "dense_dim = 128\n",
    "gru_units = 256\n",
    "num_heads = 2\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# encoder_inputs = Input(shape=(None,), dtype=\"int64\", name=\"question\")\n",
    "# x = PositionalEmbedding(max_sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "# encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x) \n",
    "# decoder_inputs = Input(shape=(None,), dtype=\"int64\", name=\"answer\")\n",
    "# x = PositionalEmbedding(max_sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "# x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs) \n",
    "# x = layers.Dropout(0.5)(x)\n",
    "# decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x) \n",
    "# transformer = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Encoder inputs\n",
    "encoder_inputs = Input(shape=(None,), dtype=\"int64\", name=\"question\")\n",
    "x = PositionalEmbedding(max_sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "x = GRU(gru_units, return_sequences=True)(x)  # Add GRU layer\n",
    "encoder_outputs = TransformerEncoder(gru_units, dense_dim, num_heads)(x)\n",
    "\n",
    "# Decoder inputs\n",
    "decoder_inputs = Input(shape=(None,), dtype=\"int64\", name=\"answer\")\n",
    "x = PositionalEmbedding(max_sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = GRU(gru_units, return_sequences=True)(x)  # Add GRU layer\n",
    "x = TransformerDecoder(gru_units, dense_dim, num_heads)(x, encoder_outputs)\n",
    "\n",
    "# Dropout and final dense layer\n",
    "x = Dropout(0.5)(x)\n",
    "decoder_outputs = Dense(vocab_size, activation=\"softmax\")(x)\n",
    "\n",
    "# Define the model\n",
    "transformer = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a16a3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.compile(\n",
    " optimizer=\"rmsprop\",\n",
    " loss=\"sparse_categorical_crossentropy\",\n",
    " metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831a229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aad9e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint('./check_model_1_2.h5',verbose=1 , save_best_only=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss'  , factor=0.1 ,patience=25 , min_lr=0.0001 , verbose=1),\n",
    "    CSVLogger('data_train.csv'),\n",
    "    TensorBoard(),\n",
    "    EarlyStopping(monitor='val_loss' ,patience=50 , restore_best_weights=False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41df8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.fit(\n",
    "    [x_train_q, x_train_m],\n",
    "    y_train_a,\n",
    "    validation_data=([x_val_q , x_val_m] , y_val_a),\n",
    "    epochs=75,\n",
    "    shuffle = True,\n",
    "    callbacks = callbacks,\n",
    "    batch_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0c44af",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.evaluate([x_test_q , x_test_m] , y_test_a , batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dff715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_single_text(text, tokenizer, max_sequence_length):\n",
    "    text = preprocess_text(text)\n",
    "    # text = f\"<START> {text} <END>\"\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    # sequence = [[start_token_index] + list(seq) + [end_token_index] for seq in sequence]\n",
    "    print(tokenizer.sequences_to_texts(sequence))\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_sequence_length, padding='post')\n",
    "    return padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70026917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_answer(question, model, tokenizer, max_sequence_length):\n",
    "    question_seq = preprocess_single_text(question, tokenizer, max_sequence_length)\n",
    "    \n",
    "    start_token = np.array([[tokenizer.word_index['<start>']]])\n",
    "    answer_seq = np.ones((1, max_sequence_length))\n",
    "    answer_seq[0, 0] = tokenizer.word_index['<start>']\n",
    "    reverse_word_index = {index: word for word, index in tokenizer.word_index.items()}\n",
    "\n",
    "\n",
    "    for i in range(1, max_sequence_length):\n",
    "        output_tokens = model.predict([question_seq, answer_seq], verbose=0 , batch_size=2)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, i-1, :])\n",
    "        answer_seq[0, i] = sampled_token_index\n",
    "\n",
    "        if sampled_token_index == tokenizer.word_index['<end>']:\n",
    "            break\n",
    "\n",
    "    answer_tokens = [reverse_word_index[int(idx)] for idx in answer_seq[0] if idx > 0]\n",
    "    \n",
    "    if '<start>' in answer_tokens:\n",
    "        answer_tokens.remove('<start>')\n",
    "    if '<end>' in answer_tokens:\n",
    "        answer_tokens.remove('<end>')\n",
    "    \n",
    "    \n",
    "    answer_tokens = [token for token in answer_tokens if token != '<OOV>']\n",
    "    answer_tokens = [token for token in answer_tokens if token != '<start>']\n",
    "\n",
    "    \n",
    "\n",
    "    answer = ' '.join(answer_tokens)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e177e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = df['Quesition'].values\n",
    "xx[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51da2ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = xx[5000]\n",
    "predicted_answer = predict_answer(question, transformer, tokenizer, max_sequence_length)\n",
    "print(\"Question: \", question)\n",
    "print(\"Answer: \", predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efe057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = xx[5]\n",
    "predicted_answer = predict_answer(question, transformer, tokenizer, max_sequence_length)\n",
    "print(\"Question: \", question)\n",
    "print(\"Answer: \", predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b60d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = xx[1]\n",
    "predicted_answer = predict_answer(question, transformer, tokenizer, max_sequence_length)\n",
    "print(\"Question: \", question)\n",
    "print(\"Answer: \", predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6af7ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what are the symptom of the cold ?\"\n",
    "predicted_answer = predict_answer(question, transformer, tokenizer, max_sequence_length)\n",
    "print(\"Question: \", question)\n",
    "print(\"Answer: \", predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200506dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is love?\"\n",
    "predicted_answer = predict_answer(question, transformer, tokenizer, max_sequence_length)\n",
    "print(\"Question: \", question)\n",
    "print(\"Answer: \", predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0476ec-ef7b-490c-8ed8-0b1ec15b539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"how are you\"\n",
    "predicted_answer = predict_answer(question, transformer, tokenizer, max_sequence_length)\n",
    "print(\"Question: \", question)\n",
    "print(\"Answer: \", predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dc069c-f5ae-4797-aaa6-7e2ddcf83668",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"hello how are you\"\n",
    "predicted_answer = predict_answer(question, transformer, tokenizer, max_sequence_length)\n",
    "print(\"Question: \", question)\n",
    "print(\"Answer: \", predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb525032-d571-4187-9ada-1f0fc2df834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what are the sign of the flu ?\"\n",
    "predicted_answer = predict_answer(question, transformer, tokenizer, max_sequence_length)\n",
    "print(\"Question: \", question)\n",
    "print(\"Answer: \", predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a125c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# transformer.save('transformer_model_with_gru.h5')\n",
    "from keras.models import load_model\n",
    "# Load the model for further training\n",
    "model1 = load_model('check_model_1_2.h5', custom_objects={'PositionalEmbedding': PositionalEmbedding, 'TransformerEncoder': TransformerEncoder, 'TransformerDecoder': TransformerDecoder})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c918ae64-9b16-425f-bebf-de2027202dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.evaluate([x_test_q , x_test_m] , y_test_a , batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afcecd2-0e59-4711-8523-6d1c6bdf9550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model again (just to be sure)\n",
    "model1.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78189975-17df-4c2e-8cd1-6f03c5159a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fit(\n",
    "    [x_test_q , x_test_m] , y_test_a,\n",
    "    epochs = 50,\n",
    "    batch_size=4,\n",
    "    validation_data=([x_val_q , x_val_m] , y_val_a),\n",
    "    callbacks = callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7da5d93-0ce6-46b7-adf8-ba81e8b0243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"how are you\"\n",
    "predicted_answer = predict_answer(question, model1, tokenizer, max_sequence_length)\n",
    "print(\"Question: \", question)\n",
    "print(\"Answer: \", predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e40403d-ff7b-4c1c-a1a9-bcd59fe31795",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"hello how are you\"\n",
    "predicted_answer = predict_answer(question, model1, tokenizer, max_sequence_length)\n",
    "print(\"Question: \", question)\n",
    "print(\"Answer: \", predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73a4f5c-be82-4945-b305-8caf92753907",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what are the sign of the flu ?\"\n",
    "predicted_answer = predict_answer(question, model1, tokenizer, max_sequence_length)\n",
    "print(\"Question: \", question)\n",
    "print(\"Answer: \", predicted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c4c999-49eb-4157-8e1d-a2221eb34ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cdd2d3-4cda-4b4e-883a-248938faae71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
